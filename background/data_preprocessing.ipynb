{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-22T14:44:45.819537Z",
     "start_time": "2025-03-22T14:44:45.384743Z"
    }
   },
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import random"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-22T14:44:45.862311Z",
     "start_time": "2025-03-22T14:44:45.858242Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Set the desired local path for your data preprocessing\n",
    "base_dir = \"/media/hghosh/HGHOSH DISK/dataset\"\n",
    "\n",
    "# Create the base directory if it doesn't exist\n",
    "os.makedirs(base_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Directory created at: {base_dir}\")\n"
   ],
   "id": "d2034bb2a2b37100",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory created at: /media/hghosh/HGHOSH DISK/dataset\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-22T14:44:46.091370Z",
     "start_time": "2025-03-22T14:44:46.087927Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Configure max images\n",
    "MAX_IMAGES = 10000"
   ],
   "id": "2a1b6cfbc3d6718c",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Function to download from URL\n",
    "def download_from_url(url, category):\n",
    "    category_dir = os.path.join(base_dir, category)\n",
    "    os.makedirs(category_dir, exist_ok=True)\n",
    "\n",
    "    # Download the file\n",
    "    filename = os.path.join(category_dir, url.split('/')[-1])\n",
    "    print(f\"Downloading {category} dataset...\")\n",
    "\n",
    "    # Skip if already downloaded\n",
    "    if os.path.exists(filename):\n",
    "        print(f\"File already exists: {filename}\")\n",
    "    else:\n",
    "        try:\n",
    "            response = requests.get(url, stream=True)\n",
    "            total_size = int(response.headers.get('content-length', 0))\n",
    "\n",
    "            with open(filename, 'wb') as f, tqdm(\n",
    "                total=total_size, unit='B', unit_scale=True, unit_divisor=1024\n",
    "            ) as bar:\n",
    "                for data in response.iter_content(chunk_size=1024):\n",
    "                    size = f.write(data)\n",
    "                    bar.update(size)\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading {url}: {e}\")\n",
    "            return False\n",
    "\n",
    "    # Extract based on file extension\n",
    "    print(f\"Extracting {category} dataset...\")\n",
    "    try:\n",
    "        if filename.endswith('.tgz') or filename.endswith('.tar.gz'):\n",
    "            with tarfile.open(filename) as tar:\n",
    "                tar.extractall(path=category_dir)\n",
    "        elif filename.endswith('.zip'):\n",
    "            with zipfile.ZipFile(filename) as zip_ref:\n",
    "                zip_ref.extractall(category_dir)\n",
    "\n",
    "        # Remove the compressed file to save space\n",
    "        # Uncomment if you want to keep the original archives\n",
    "        # os.remove(filename)\n",
    "\n",
    "        print(f\"{category} dataset extracted.\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting {filename}: {e}\")\n",
    "        return False"
   ],
   "id": "3fba603795ada3d5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Function to download from Kaggle\n",
    "def download_from_kaggle(dataset, category):\n",
    "    category_dir = os.path.join(base_dir, category)\n",
    "    os.makedirs(category_dir, exist_ok=True)\n",
    "\n",
    "    # Check if kaggle CLI is installed\n",
    "    try:\n",
    "        subprocess.run([\"kaggle\", \"--version\"], check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    except (subprocess.SubprocessError, FileNotFoundError):\n",
    "        print(\"Kaggle CLI not found. Please install it with: pip install kaggle\")\n",
    "        print(\"And set up your API credentials: https://github.com/Kaggle/kaggle-api#api-credentials\")\n",
    "        return False\n",
    "\n",
    "    # Check if API credentials are set up\n",
    "    kaggle_dir = os.path.join(os.path.expanduser(\"~\"), \".kaggle\")\n",
    "    kaggle_json = os.path.join(kaggle_dir, \"kaggle.json\")\n",
    "\n",
    "    if not os.path.exists(kaggle_json):\n",
    "        print(\"Kaggle API credentials not found.\")\n",
    "        print(\"Please create a token at https://www.kaggle.com/settings/account\")\n",
    "        print(f\"Then create {kaggle_json} with your API key and username.\")\n",
    "        return False\n",
    "\n",
    "    # Make sure permissions are correct\n",
    "    try:\n",
    "        os.chmod(kaggle_json, 0o600)\n",
    "    except:\n",
    "        print(f\"Warning: Could not set permissions on {kaggle_json}\")\n",
    "\n",
    "    # Download the dataset\n",
    "    try:\n",
    "        print(f\"Downloading {category} dataset from Kaggle...\")\n",
    "        subprocess.run(\n",
    "            [\"kaggle\", \"datasets\", \"download\", \"-d\", dataset, \"-p\", category_dir],\n",
    "            check=True\n",
    "        )\n",
    "\n",
    "        # Find the downloaded zip file\n",
    "        zip_files = [f for f in os.listdir(category_dir) if f.endswith('.zip')]\n",
    "        if not zip_files:\n",
    "            print(\"No zip file found after download.\")\n",
    "            return False\n",
    "\n",
    "        zip_file = os.path.join(category_dir, zip_files[0])\n",
    "\n",
    "        # Extract the zip file\n",
    "        print(f\"Extracting {zip_file}...\")\n",
    "        with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "            zip_ref.extractall(category_dir)\n",
    "\n",
    "        # Remove the zip file\n",
    "        os.remove(zip_file)\n",
    "        print(f\"{category} dataset extracted.\")\n",
    "        return True\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error downloading from Kaggle: {e}\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing Kaggle dataset: {e}\")\n",
    "        return False\n"
   ],
   "id": "861a814ba9533ca4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-22T14:44:46.162739Z",
     "start_time": "2025-03-22T14:44:46.147485Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def preprocess_dataset(category):\n",
    "    print(f\"Preprocessing {category} dataset...\")\n",
    "\n",
    "    # Define directories\n",
    "    category_dir = os.path.join(base_dir, category)\n",
    "    processed_dir = os.path.join(base_dir, f\"{category}_processed\")\n",
    "    pairs_dir = os.path.join(base_dir, f\"{category}_pairs\")\n",
    "    # Check if the dataset has already been processed\n",
    "    if os.path.exists(processed_dir) and os.path.exists(pairs_dir):\n",
    "        print(f\"{category} dataset already processed. Skipping...\")\n",
    "        return\n",
    "\n",
    "    # Create output directories\n",
    "    os.makedirs(processed_dir, exist_ok=True)\n",
    "    os.makedirs(pairs_dir, exist_ok=True)\n",
    "    os.makedirs(os.path.join(pairs_dir, \"train\"), exist_ok=True)\n",
    "    os.makedirs(os.path.join(pairs_dir, \"val\"), exist_ok=True)\n",
    "\n",
    "    # Find all image files recursively in the category directory\n",
    "    image_files = []\n",
    "    for root, _, files in os.walk(category_dir):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                image_files.append(os.path.join(root, file))\n",
    "\n",
    "    print(f\"Found {len(image_files)} images for {category}\")\n",
    "\n",
    "    # If more than MAX_IMAGES, randomly select MAX_IMAGES\n",
    "    if len(image_files) > MAX_IMAGES:\n",
    "        print(f\"Limiting to {MAX_IMAGES} random images\")\n",
    "        random.shuffle(image_files)\n",
    "        image_files = image_files[:MAX_IMAGES]\n",
    "\n",
    "    # Process each image\n",
    "    processed_count = 0\n",
    "    for img_path in tqdm(image_files, desc=f\"Processing {category} images\"):\n",
    "        try:\n",
    "            # Read the image\n",
    "            img = cv2.imread(img_path)\n",
    "            if img is None:\n",
    "                continue\n",
    "\n",
    "            # Resize to 256x256\n",
    "            img = cv2.resize(img, (256, 256))\n",
    "\n",
    "            # Create sketch using Canny edge detection\n",
    "            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "            edges = cv2.Canny(gray, 100, 200)\n",
    "\n",
    "            # Convert edges to 3-channel image\n",
    "            sketch = cv2.cvtColor(edges, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "            # Normalize images to range [-1, 1]\n",
    "            img_norm = (img / 127.5) - 1\n",
    "            sketch_norm = (sketch / 127.5) - 1\n",
    "\n",
    "            # Save the original and sketch as a pair\n",
    "            filename = os.path.basename(img_path).split('.')[0] + '.jpg'\n",
    "\n",
    "            # Decide if this goes to train or validation (80/20 split)\n",
    "            if np.random.random() < 0.8:\n",
    "                split = \"train\"\n",
    "            else:\n",
    "                split = \"val\"\n",
    "\n",
    "            pair_dir = os.path.join(pairs_dir, split)\n",
    "\n",
    "            # Save the pair side by side\n",
    "            pair = np.concatenate([sketch, img], axis=1)\n",
    "            cv2.imwrite(os.path.join(pair_dir, filename), pair)\n",
    "\n",
    "            # Also save individual files for flexibility\n",
    "            cv2.imwrite(os.path.join(pair_dir, f\"{filename.split('.')[0]}_sketch.jpg\"), sketch)\n",
    "            cv2.imwrite(os.path.join(pair_dir, f\"{filename.split('.')[0]}_real.jpg\"), img)\n",
    "\n",
    "            processed_count += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {img_path}: {e}\")\n",
    "\n",
    "    print(f\"Successfully processed {processed_count} images for {category}\")\n",
    "    return processed_count"
   ],
   "id": "c727974146b0492b",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-22T14:44:46.241225Z",
     "start_time": "2025-03-22T14:44:46.234109Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def verify_image_counts():\n",
    "    \"\"\"Verify the total number of processed images and report counts\"\"\"\n",
    "    print(\"\\n--- Dataset Statistics ---\")\n",
    "\n",
    "    total_images = 0\n",
    "    for category in os.listdir(base_dir):\n",
    "        if category.endswith(\"_pairs\"):\n",
    "            pairs_dir = os.path.join(base_dir, category)\n",
    "            train_dir = os.path.join(pairs_dir, \"train\")\n",
    "            val_dir = os.path.join(pairs_dir, \"val\")\n",
    "\n",
    "            if not os.path.exists(train_dir) or not os.path.exists(val_dir):\n",
    "                print(f\"Warning: Directories for {category} not found\")\n",
    "                continue\n",
    "\n",
    "            train_count = len([f for f in os.listdir(train_dir) if f.endswith('.jpg') and not (f.endswith('_sketch.jpg') or f.endswith('_real.jpg'))])\n",
    "            val_count = len([f for f in os.listdir(val_dir) if f.endswith('.jpg') and not (f.endswith('_sketch.jpg') or f.endswith('_real.jpg'))])\n",
    "\n",
    "            print(f\"{category[:-6]} dataset: {train_count} training images, {val_count} validation images, {train_count + val_count} total\")\n",
    "            total_images += train_count + val_count\n",
    "\n",
    "    print(f\"\\nTotal images across all datasets: {total_images}\")"
   ],
   "id": "a402408a0f0c8bb0",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-22T15:22:23.056290Z",
     "start_time": "2025-03-22T14:44:46.286764Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # List of categories to process\n",
    "    all_categories = []\n",
    "\n",
    "    # Manually specify categories if they exist locally\n",
    "    for category in os.listdir(base_dir):\n",
    "        if not category.endswith(\"_pairs\") and not category.endswith(\"_processed\"):\n",
    "            all_categories.append(category)\n",
    "\n",
    "    # Process each dataset\n",
    "    for category in all_categories:\n",
    "        preprocess_dataset(category)\n",
    "\n",
    "    # Verify the counts\n",
    "    verify_image_counts()\n",
    "\n",
    "    print(\"\\nAll datasets processed and organized!\")\n",
    "    print(f\"Data is stored in: {base_dir}\")"
   ],
   "id": "a89578196a59c910",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing birds.pth dataset...\n",
      "Found 0 images for birds.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing birds.pth images: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed 0 images for birds.pth\n",
      "Preprocessing cat dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 29843 images for cat\n",
      "Limiting to 10000 random images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing cat images: 100%|██████████| 10000/10000 [09:10<00:00, 18.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed 10000 images for cat\n",
      "Preprocessing CUB_200_2011 dataset...\n",
      "Found 11788 images for CUB_200_2011\n",
      "Limiting to 10000 random images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CUB_200_2011 images: 100%|██████████| 10000/10000 [05:35<00:00, 29.84it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed 10000 images for CUB_200_2011\n",
      "Preprocessing face dataset...\n",
      "Found 202599 images for face\n",
      "Limiting to 10000 random images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing face images: 100%|██████████| 10000/10000 [12:49<00:00, 12.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed 10000 images for face\n",
      "Preprocessing shoes dataset...\n",
      "Found 100091 images for shoes\n",
      "Limiting to 10000 random images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing shoes images: 100%|██████████| 10000/10000 [07:20<00:00, 22.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed 10000 images for shoes\n",
      "Preprocessing cats.pth dataset...\n",
      "Found 0 images for cats.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing cats.pth images: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed 0 images for cats.pth\n",
      "Preprocessing faces.pth dataset...\n",
      "Found 0 images for faces.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing faces.pth images: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed 0 images for faces.pth\n",
      "Preprocessing shoes_pairs.zip dataset...\n",
      "Found 0 images for shoes_pairs.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing shoes_pairs.zip images: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed 0 images for shoes_pairs.zip\n",
      "Preprocessing flower.pth dataset...\n",
      "Found 0 images for flower.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing flower.pth images: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed 0 images for flower.pth\n",
      "\n",
      "--- Dataset Statistics ---\n",
      "birds.pth dataset: 0 training images, 0 validation images, 0 total\n",
      "cat dataset: 8024 training images, 1976 validation images, 10000 total\n",
      "CUB_200_2011 dataset: 7967 training images, 2033 validation images, 10000 total\n",
      "face dataset: 8019 training images, 1981 validation images, 10000 total\n",
      "shoes dataset: 6658 training images, 1920 validation images, 8578 total\n",
      "cats.pth dataset: 0 training images, 0 validation images, 0 total\n",
      "faces.pth dataset: 0 training images, 0 validation images, 0 total\n",
      "shoes_pairs.zip dataset: 0 training images, 0 validation images, 0 total\n",
      "flower.pth dataset: 0 training images, 0 validation images, 0 total\n",
      "\n",
      "Total images across all datasets: 38578\n",
      "\n",
      "All datasets processed and organized!\n",
      "Data is stored in: /media/hghosh/HGHOSH DISK/dataset\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "https://mitmedialab.github.io/GAN-play/",
   "id": "163c145c7558ffec"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-22T15:22:33.874752Z",
     "start_time": "2025-03-22T15:22:23.145536Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "def delete_excess_images(category, max_images):\n",
    "    pairs_dir = os.path.join(base_dir, f\"{category}_pairs\")\n",
    "    train_dir = os.path.join(pairs_dir, \"train\")\n",
    "    val_dir = os.path.join(pairs_dir, \"val\")\n",
    "\n",
    "    # Get all image files in train and val directories\n",
    "    train_images = [f for f in os.listdir(train_dir) if f.endswith('.jpg') and not (f.endswith('_sketch.jpg') or f.endswith('_real.jpg'))]\n",
    "    val_images = [f for f in os.listdir(val_dir) if f.endswith('.jpg') and not (f.endswith('_sketch.jpg') or f.endswith('_real.jpg'))]\n",
    "\n",
    "    total_images = train_images + val_images\n",
    "\n",
    "    # If total images exceed max_images, delete the excess one by one\n",
    "    if len(total_images) > max_images:\n",
    "        print(f\"Deleting excess images for {category}...\")\n",
    "\n",
    "        # Randomly shuffle and select images to delete\n",
    "        random.shuffle(total_images)\n",
    "        images_to_delete = total_images[max_images:]\n",
    "\n",
    "        for img in images_to_delete:\n",
    "            if img in train_images:\n",
    "                os.remove(os.path.join(train_dir, img))\n",
    "                os.remove(os.path.join(train_dir, f\"{img.split('.')[0]}_sketch.jpg\"))\n",
    "                os.remove(os.path.join(train_dir, f\"{img.split('.')[0]}_real.jpg\"))\n",
    "            else:\n",
    "                os.remove(os.path.join(val_dir, img))\n",
    "                os.remove(os.path.join(val_dir, f\"{img.split('.')[0]}_sketch.jpg\"))\n",
    "                os.remove(os.path.join(val_dir, f\"{img.split('.')[0]}_real.jpg\"))\n",
    "\n",
    "            print(f\"Deleted {img}\")\n",
    "\n",
    "        print(f\"Deleted {len(images_to_delete)} images for {category}\")\n",
    "\n",
    "# Call delete_excess_images after preprocessing each category\n",
    "for category in all_categories:\n",
    "    preprocess_dataset(category)\n",
    "    delete_excess_images(category, MAX_IMAGES)\n",
    "\n",
    "# Verify the counts\n",
    "verify_image_counts()"
   ],
   "id": "5ed9a0a80fda58be",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing birds.pth dataset...\n",
      "birds.pth dataset already processed. Skipping...\n",
      "Preprocessing cat dataset...\n",
      "cat dataset already processed. Skipping...\n",
      "Preprocessing CUB_200_2011 dataset...\n",
      "CUB_200_2011 dataset already processed. Skipping...\n",
      "Preprocessing face dataset...\n",
      "face dataset already processed. Skipping...\n",
      "Preprocessing shoes dataset...\n",
      "shoes dataset already processed. Skipping...\n",
      "Preprocessing cats.pth dataset...\n",
      "cats.pth dataset already processed. Skipping...\n",
      "Preprocessing faces.pth dataset...\n",
      "faces.pth dataset already processed. Skipping...\n",
      "Preprocessing shoes_pairs.zip dataset...\n",
      "shoes_pairs.zip dataset already processed. Skipping...\n",
      "Preprocessing flower.pth dataset...\n",
      "flower.pth dataset already processed. Skipping...\n",
      "\n",
      "--- Dataset Statistics ---\n",
      "birds.pth dataset: 0 training images, 0 validation images, 0 total\n",
      "cat dataset: 8024 training images, 1976 validation images, 10000 total\n",
      "CUB_200_2011 dataset: 7967 training images, 2033 validation images, 10000 total\n",
      "face dataset: 8019 training images, 1981 validation images, 10000 total\n",
      "shoes dataset: 6658 training images, 1920 validation images, 8578 total\n",
      "cats.pth dataset: 0 training images, 0 validation images, 0 total\n",
      "faces.pth dataset: 0 training images, 0 validation images, 0 total\n",
      "shoes_pairs.zip dataset: 0 training images, 0 validation images, 0 total\n",
      "flower.pth dataset: 0 training images, 0 validation images, 0 total\n",
      "\n",
      "Total images across all datasets: 38578\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-22T15:22:33.882198Z",
     "start_time": "2025-03-22T15:22:33.880080Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "c38e7cc1baa17765",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
